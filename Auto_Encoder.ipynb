{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9c0416e9-7849-4493-bcbd-655f6252cfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import exp\n",
    "from math import log\n",
    "np.random.seed(1)\n",
    "def logistic_function(a,W):\n",
    "    b=a.dot(W)\n",
    "    return 1/(1+np.exp(b))\n",
    "def linear_function(a,W):\n",
    "    return a.dot(W)\n",
    "def loss(x,inp):\n",
    "    output=0\n",
    "    for i in range(len(x)):\n",
    "        output-=(inp[i]*log(x[i])+(1-inp[i])*log(1-x[i]))\n",
    "    return output\n",
    "def der_loss(x,inp):\n",
    "    output=[]\n",
    "    for i in range(len(x)):\n",
    "        output.append(-(inp[i]/x[i])+(1-inp[i])/(2-x[i]))\n",
    "    return output\n",
    "import numpy as np\n",
    "input=np.array([0,1,1,0,1])\n",
    "weight_0_1=np.random.randn(5,3)\n",
    "weight_1_2=np.random.randn(3,5)\n",
    "for epoch in range(60):\n",
    "    output_1=linear_function(input,weight_0_1)\n",
    "    output_2=logistic_function(output_1,weight_1_2)\n",
    "    loss_1=loss(output_2,input)\n",
    "    delta_2="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "52068ee1-612d-4dd4-bc6b-a94f06251453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.8236424541417632\n",
      "Epoch 10, Loss: 9.992007221626415e-16\n",
      "Epoch 20, Loss: 9.992007221626415e-16\n",
      "Epoch 30, Loss: 9.992007221626415e-16\n",
      "Epoch 40, Loss: 9.992007221626415e-16\n",
      "Epoch 50, Loss: 9.992007221626415e-16\n",
      "Final output: [0. 1. 1. 0. 1.]\n",
      "[[ 1.62434536e+00 -6.11756414e-01 -5.28171752e-01]\n",
      " [ 5.11752295e+14  9.03080777e+14 -1.02396892e+14]\n",
      " [ 5.11752295e+14  9.03080777e+14 -1.02396892e+14]\n",
      " [-2.49370375e-01  1.46210794e+00 -2.06014071e+00]\n",
      " [ 5.11752295e+14  9.03080777e+14 -1.02396892e+14]]\n",
      "[[-3.96401623e+14  3.96401623e+14  3.96401623e+14 -3.96401623e+14\n",
      "   3.96401623e+14]\n",
      " [-6.99523362e+14  6.99523362e+14  6.99523362e+14 -6.99523362e+14\n",
      "   6.99523362e+14]\n",
      " [ 7.93162915e+13 -7.93162915e+13 -7.93162915e+13  7.93162915e+13\n",
      "  -7.93162915e+13]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_11672\\2996363514.py:6: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-a.dot(W)))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "# Vectorized sigmoid\n",
    "def logistic_function(a, W):\n",
    "    return 1 / (1 + np.exp(-a.dot(W)))\n",
    "\n",
    "# Linear activation\n",
    "def linear_function(a, W):\n",
    "    return a.dot(W)\n",
    "\n",
    "# Binary cross-entropy loss\n",
    "def loss(x, inp):\n",
    "    x = np.clip(x, 1e-15, 1 - 1e-15)  # Avoid log(0)\n",
    "    return -np.mean(inp * np.log(x) + (1 - inp) * np.log(1 - x))\n",
    "\n",
    "# Derivative of loss\n",
    "def der_loss(x, inp):\n",
    "    x = np.clip(x, 1e-15, 1 - 1e-15)\n",
    "    return -(inp / x - (1 - inp) / (1 - x))\n",
    "\n",
    "alpha = 0.2\n",
    "input_data = np.array([0, 1, 1, 0, 1])  # Shape (5,)\n",
    "weight_0_1 = np.random.randn(5, 3)      # Shape (5, 3)\n",
    "weight_1_2 = np.random.randn(3, 5)      # Shape (3, 5)\n",
    "\n",
    "for epoch in range(60):\n",
    "    # Forward pass\n",
    "    output_1 = linear_function(input_data, weight_0_1)  # Shape (3,)\n",
    "    output_2 = logistic_function(output_1, weight_1_2)  # Shape (5,)\n",
    "    \n",
    "    # Loss\n",
    "    loss_1 = loss(output_2, input_data)\n",
    "    \n",
    "    # Gradients\n",
    "    delta_2 = der_loss(output_2, input_data)  # Shape (5,)\n",
    "    grad_weight_1_2 = np.outer(output_1, delta_2)  # Shape (3, 5)\n",
    "    delta_1 = delta_2.dot(weight_1_2.T)  # Shape (3,)\n",
    "    grad_weight_0_1 = np.outer(input_data, delta_1)  # Shape (5, 3)\n",
    "    \n",
    "    # Batch GD: Single update per epoch\n",
    "    weight_1_2 -= alpha * grad_weight_1_2\n",
    "    weight_0_1 -= alpha * grad_weight_0_1\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss_1}\")\n",
    "\n",
    "print(\"Final output:\", output_2)\n",
    "print(weight_0_1)\n",
    "print(weight_1_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "12610417-6ebd-416a-aefc-38eaf5dba883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99999998 1.99999998 3.00000001 4.00000003 4.99999998]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "data=np.array([1,2,3,4,5])\n",
    "weight_0_1=np.random.randn(5,3)\n",
    "weight_1_2=np.random.randn(3,5)\n",
    "for epoch in range(60):\n",
    "    layer_1=data.dot(weight_0_1)\n",
    "    layer_2=layer_1.dot(weight_1_2)\n",
    "    delta_2=2*(layer_2-data)/5\n",
    "    grad_2=np.outer(layer_1,delta_2)\n",
    "    \n",
    "    weight_1_2-=0.01*grad_2\n",
    "    delta_1=delta_2.dot(weight_1_2.T)        \n",
    "    grad_1=np.outer(data,delta_1)\n",
    "    weight_0_1-=0.01*grad_1\n",
    "print(layer_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "54be878b-20db-404f-83f2-136ee8fbb9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 9.102275669731629\n",
      "Epoch 10, Loss: 0.027486968014934655\n",
      "Epoch 20, Loss: 4.573334757377309e-05\n",
      "Epoch 30, Loss: 1.3915039667685855e-07\n",
      "Epoch 40, Loss: 4.960333410897945e-10\n",
      "Epoch 50, Loss: 1.8044274394472322e-12\n",
      "Final weight_0_1:\n",
      " [[ 1.65351765 -0.62264862 -0.52440159]\n",
      " [-1.01462406  0.84362321 -2.29399838]\n",
      " [ 1.83232861 -0.79388353  0.33034957]\n",
      " [-0.13268124  1.4185391  -2.04506007]\n",
      " [-0.17655579 -0.4385154   1.15262024]]\n",
      "Final weight_1_2:\n",
      " [[-0.93291789 -0.20163456 -0.96415419  0.0973619   0.81391902]\n",
      " [-0.97986837  1.11612301  0.82286257  0.54394741  1.08144821]\n",
      " [-1.0063455  -0.0507354  -0.73315661 -0.37648115  0.05484951]]\n",
      "Final output: [0.9999999  1.99999989 3.00000005 4.00000016 4.9999999 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "# Linear activation (identity)\n",
    "def linear_function(a, W):\n",
    "    return a.dot(W)\n",
    "\n",
    "# Mean squared error loss\n",
    "def mse_loss(pred, true):\n",
    "    return np.mean((pred - true) ** 2)\n",
    "\n",
    "alpha = 0.01  # Learning rate\n",
    "data = np.array([1, 2, 3, 4, 5])  # Shape (5,)\n",
    "weight_0_1 = np.random.randn(5, 3)  # Shape (5, 3)\n",
    "weight_1_2 = np.random.randn(3, 5)  # Shape (3, 5)\n",
    "\n",
    "for epoch in range(60):\n",
    "    # Forward pass\n",
    "    layer_1 = linear_function(data, weight_0_1)  # Shape (3,)\n",
    "    layer_2 = linear_function(layer_1, weight_1_2)  # Shape (5,)\n",
    "    \n",
    "    # Loss\n",
    "    loss = mse_loss(layer_2, data)\n",
    "    \n",
    "    # Gradients\n",
    "    delta_2 = 2 * (layer_2 - data) / len(data)  # Shape (5,)\n",
    "    grad_weight_1_2 = np.outer(layer_1, delta_2)  # Shape (3, 5)\n",
    "    delta_1 = delta_2.dot(weight_1_2.T)  # Shape (3,)\n",
    "    grad_weight_0_1 = np.outer(data, delta_1)  # Shape (5, 3)\n",
    "    \n",
    "    # Batch GD: Single update per epoch\n",
    "    weight_1_2 -= alpha * grad_weight_1_2\n",
    "    weight_0_1 -= alpha * grad_weight_0_1\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "print(\"Final weight_0_1:\\n\", weight_0_1)\n",
    "print(\"Final weight_1_2:\\n\", weight_1_2)\n",
    "print(\"Final output:\", layer_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "9ed5074b-1c8e-4a9b-a7cf-54d7beb532a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 9.102275669731629\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "weight_0_1:\n",
      " [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "weight_1_2:\n",
      " [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "[nan nan nan nan nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_11672\\3255158237.py:14: RuntimeWarning: overflow encountered in square\n",
      "  loss = np.mean((layer_2 - data) ** 2)  # Scalar MSE\n",
      "C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_11672\\3255158237.py:19: RuntimeWarning: invalid value encountered in subtract\n",
      "  weight_1_2 -= 0.2 * grad_2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "data = np.array([1, 2, 3, 4, 5])\n",
    "weight_0_1 = np.random.randn(5, 3)\n",
    "weight_1_2 = np.random.randn(3, 5)\n",
    "\n",
    "for epoch in range(60):\n",
    "    # Forward pass\n",
    "    layer_1 = data.dot(weight_0_1)  # Shape (3,)\n",
    "    layer_2 = layer_1.dot(weight_1_2)  # Shape (5,)\n",
    "    \n",
    "    # Loss\n",
    "    loss = np.mean((layer_2 - data) ** 2)  # Scalar MSE\n",
    "    \n",
    "    # Gradients\n",
    "    delta_2 = 2 * (layer_2 - data) / len(data)  # Shape (5,)\n",
    "    grad_2 = np.outer(layer_1, delta_2)  # Shape (3, 5)\n",
    "    weight_1_2 -= 0.2 * grad_2\n",
    "    delta_1 = delta_2.dot(weight_1_2.T)  # Shape (3,)\n",
    "    grad_1 = np.outer(data, delta_1)  # Shape (5, 3)\n",
    "    weight_0_1 -= 0.2 * grad_1\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "print(\"weight_0_1:\\n\", weight_0_1)\n",
    "print(\"weight_1_2:\\n\", weight_1_2)\n",
    "print(layer_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8a18d592-79f4-428c-8785-ecf522fddb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.3640910267892653\n",
      "Epoch 10, Loss: 0.25748149297317025\n",
      "Epoch 20, Loss: 0.19798629931670925\n",
      "Epoch 30, Loss: 0.15985627169698144\n",
      "Epoch 40, Loss: 0.13245385520523345\n",
      "Epoch 50, Loss: 0.11108450083237069\n",
      "weight_0_1:\n",
      " [[ 1.64315443 -0.62124459 -0.5137946 ]\n",
      " [-1.03535049  0.84643128 -2.2727844 ]\n",
      " [ 1.80123897 -0.78967142  0.36217054]\n",
      " [-0.17413411  1.42415525 -2.00263211]\n",
      " [-0.22837187 -0.43149522  1.20565519]]\n",
      "weight_1_2:\n",
      " [[-1.04629771 -0.19257724 -0.92631303  0.06418301  0.67489695]\n",
      " [-1.05412375  1.12585047  0.8561924   0.52157353  0.98337037]\n",
      " [-0.80235204 -0.0752656  -0.82121302 -0.31655597  0.32082793]]\n",
      "Final output: [-0.17582035  0.47486327  0.78353297  0.6499968   0.47766882]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "# Normalize input to [0, 1]\n",
    "data = np.array([1, 2, 3, 4, 5]) / 5.0  # Shape (5,)\n",
    "weight_0_1 = np.random.randn(5, 3)       # Shape (5, 3)\n",
    "weight_1_2 = np.random.randn(3, 5)       # Shape (3, 5)\n",
    "\n",
    "for epoch in range(60):\n",
    "    # Forward pass\n",
    "    layer_1 = data.dot(weight_0_1)  # Shape (3,)\n",
    "    layer_2 = layer_1.dot(weight_1_2)  # Shape (5,)\n",
    "    \n",
    "    # Loss\n",
    "    loss = np.mean((layer_2 - data) ** 2)\n",
    "    \n",
    "    # Gradients with clipping\n",
    "    delta_2 = 2 * (layer_2 - data) / len(data)  # Shape (5,)\n",
    "    delta_2 = np.clip(delta_2, -1, 1)  # Clip to prevent explosion\n",
    "    grad_2 = np.outer(layer_1, delta_2)  # Shape (3, 5)\n",
    "    weight_1_2 -= 0.01 * grad_2  # Smaller learning rate\n",
    "    delta_1 = delta_2.dot(weight_1_2.T)  # Shape (3,)\n",
    "    delta_1 = np.clip(delta_1, -1, 1)  # Clip\n",
    "    grad_1 = np.outer(data, delta_1)  # Shape (5, 3)\n",
    "    weight_0_1 -= 0.01 * grad_1\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "print(\"weight_0_1:\\n\", weight_0_1)\n",
    "print(\"weight_1_2:\\n\", weight_1_2)\n",
    "print(\"Final output:\", layer_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c9b55a-9f19-48cd-9720-d0761b205b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

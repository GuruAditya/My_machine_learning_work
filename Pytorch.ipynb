{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "da2ae943-5d8f-4330-bd36-10eec9fec53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU time: 9.163564205169678 seconds\n",
      "GPU time: 0.007004261016845703 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(5)\n",
    "\n",
    "# Create tensors\n",
    "a = torch.randint(low=0, high=100, size=(1000, 1000))  # 1000x1000 tensor of integers [0, 100)\n",
    "b = torch.randint(low=0, high=100, size=(1000, 1000))  # 1000x1000 tensor of integers [0, 100)\n",
    "\n",
    "# CPU matrix multiplication\n",
    "start_time = time.time()\n",
    "multiply = torch.matmul(a, b)\n",
    "print(\"CPU time:\", time.time() - start_time, \"seconds\")\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    # Cast tensors to float32 before moving to GPU\n",
    "    a = a.to(device, dtype=torch.float32)\n",
    "    b = b.to(device, dtype=torch.float32)\n",
    "    \n",
    "    # GPU matrix multiplication\n",
    "    start_time = time.time()\n",
    "    multiply = torch.matmul(a, b)\n",
    "    torch.cuda.synchronize()  # Ensure GPU operations are complete\n",
    "    print(\"GPU time:\", time.time() - start_time, \"seconds\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Skipping GPU computation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "36ce2136-314d-47a7-886c-f12007bcaf14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3., requires_grad=True) tensor(9., grad_fn=<PowBackward0>) tensor(0.4121, grad_fn=<SinBackward0>)\n",
      "tensor(-5.4668)\n"
     ]
    }
   ],
   "source": [
    "#autograd\n",
    "x=torch.tensor(3.0,requires_grad=True)\n",
    "y=x**2\n",
    "z=torch.sin(y)\n",
    "print(x,y,z)\n",
    "z.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ec5a46c2-8522-43ce-a95a-2750f0ace8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.7824)\n",
      "tensor(0.9989)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cross_entropy_loss(y_pred,y):\n",
    "    epsilon=1e-8\n",
    "    prediction=torch.clamp(y_pred,epsilon,1-epsilon)\n",
    "    return -(y*torch.log(prediction)+(1-y)*torch.log(1-prediction))\n",
    "x=torch.tensor(6.79)\n",
    "y=torch.tensor(0.0)\n",
    "w=torch.tensor(1.0,requires_grad=True)\n",
    "b=torch.tensor(0.0,requires_grad=True)\n",
    "z=w*x+b\n",
    "y_pred=torch.sigmoid(z)\n",
    "loss=cross_entropy_loss(y_pred,y)\n",
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)\n",
    "w.grad.zero_()\n",
    "b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c160c90-ddbe-4413-b35a-c08863fe4749",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to stop the gradient calculations we can use\n",
    "#requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "83b43ca8-2e4d-4874-95f0-9b1ec93647a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([nan], grad_fn=<AddBackward0>)\n",
      "tensor([nan], grad_fn=<AddBackward0>)\n",
      "tensor([nan], grad_fn=<AddBackward0>)\n",
      "tensor([nan], grad_fn=<AddBackward0>)\n",
      "tensor([nan], grad_fn=<AddBackward0>)\n",
      "tensor([nan], grad_fn=<AddBackward0>)\n",
      "tensor([nan], grad_fn=<AddBackward0>)\n",
      "tensor([nan], grad_fn=<AddBackward0>)\n",
      "tensor([nan], grad_fn=<AddBackward0>)\n",
      "tensor([nan], grad_fn=<AddBackward0>)\n",
      "tensor([nan], grad_fn=<AddBackward0>)\n",
      "tensor([nan], grad_fn=<AddBackward0>)\n",
      "tensor([nan], grad_fn=<AddBackward0>)\n",
      "tensor([nan], grad_fn=<AddBackward0>)\n",
      "tensor([nan], grad_fn=<AddBackward0>)\n",
      "tensor([nan], grad_fn=<AddBackward0>)\n",
      "tensor([nan], grad_fn=<AddBackward0>)\n",
      "tensor([nan], grad_fn=<AddBackward0>)\n",
      "tensor([nan], grad_fn=<AddBackward0>)\n",
      "tensor([nan], grad_fn=<AddBackward0>)\n",
      "tensor([nan], grad_fn=<AddBackward0>)\n",
      "tensor([nan], grad_fn=<AddBackward0>)\n",
      "tensor([nan], grad_fn=<AddBackward0>)\n",
      "tensor([nan], grad_fn=<AddBackward0>)\n",
      "tensor([nan], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/gscdit/Breast-Cancer-Detection/refs/heads/master/data.csv')\n",
    "df.drop(columns=[\"id\",\"Unnamed: 32\"],inplace=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, 1:], df.iloc[:, 0], test_size=0.2)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "encoder=LabelEncoder()\n",
    "y_train=encoder.fit_transform(y_train)\n",
    "y_test=encoder.transform(y_test)\n",
    "X_train_tensor=torch.from_numpy(X_train).float()\n",
    "X_test_tensor=torch.from_numpy(X_test).float()\n",
    "y_train_tensor=torch.from_numpy(y_train).float()\n",
    "y_test_tensor=torch.from_numpy(y_test).float()\n",
    "learning_rate=0.1\n",
    "epoches=25\n",
    "weights=torch.rand(df.shape[1]-1,1,requires_grad=True)\n",
    "bias=torch.rand(1,requires_grad=True)\n",
    "\n",
    "def cross_entropy_loss(y_pred,y):\n",
    "    epsilon=1e-8\n",
    "    prediction=torch.clamp(y_pred,epsilon,1-epsilon)\n",
    "    return -(y*torch.log(prediction)+(1-y)*torch.log(1-prediction))\n",
    "for epoch in range(epoches):\n",
    "    loss=0\n",
    "    for i in range(X_train.shape[0]):\n",
    "        z=torch.matmul(X_train_tensor[i],weights)+bias\n",
    "        y_pred=torch.sigmoid(z)\n",
    "        loss+=cross_entropy_loss(y_pred,y_train_tensor[i])\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        weights -= learning_rate * weights.grad\n",
    "        bias -= learning_rate * bias.grad\n",
    "    weights.grad.zero_()\n",
    "    bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94e58839-adbf-4d1e-8cb7-42b9a002db43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([400.3140], grad_fn=<AddBackward0>)\n",
      "tensor([nan], grad_fn=<AddBackward0>)\n",
      "tensor([nan], grad_fn=<AddBackward0>)\n",
      "tensor([nan], grad_fn=<AddBackward0>)\n",
      "tensor([nan], grad_fn=<AddBackward0>)\n",
      "tensor([nan], grad_fn=<AddBackward0>)\n",
      "tensor([nan], grad_fn=<AddBackward0>)\n",
      "tensor([nan], grad_fn=<AddBackward0>)\n",
      "tensor([nan], grad_fn=<AddBackward0>)\n",
      "tensor([nan], grad_fn=<AddBackward0>)\n",
      "tensor([nan], grad_fn=<AddBackward0>)\n",
      "tensor([nan], grad_fn=<AddBackward0>)\n",
      "tensor([nan], grad_fn=<AddBackward0>)\n",
      "tensor([nan], grad_fn=<AddBackward0>)\n",
      "tensor([nan], grad_fn=<AddBackward0>)\n",
      "tensor([nan], grad_fn=<AddBackward0>)\n",
      "tensor([nan], grad_fn=<AddBackward0>)\n",
      "tensor([nan], grad_fn=<AddBackward0>)\n",
      "tensor([nan], grad_fn=<AddBackward0>)\n",
      "tensor([nan], grad_fn=<AddBackward0>)\n",
      "tensor([nan], grad_fn=<AddBackward0>)\n",
      "tensor([nan], grad_fn=<AddBackward0>)\n",
      "tensor([nan], grad_fn=<AddBackward0>)\n",
      "tensor([nan], grad_fn=<AddBackward0>)\n",
      "tensor([nan], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Load and preprocess the data\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/gscdit/Breast-Cancer-Detection/refs/heads/master/data.csv')\n",
    "df.drop(columns=[\"id\", \"Unnamed: 32\"], inplace=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, 1:], df.iloc[:, 0], test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "X_train_tensor = torch.from_numpy(X_train).float()\n",
    "X_test_tensor = torch.from_numpy(X_test).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).float().unsqueeze(1)  # Make it (n_samples, 1)\n",
    "y_test_tensor = torch.from_numpy(y_test).float().unsqueeze(1)\n",
    "\n",
    "# Initialize weights and bias\n",
    "learning_rate = 0.1\n",
    "epochs = 25\n",
    "weights = torch.randn(X_train.shape[1], 1, requires_grad=True)\n",
    "bias = torch.randn(1, requires_grad=True)\n",
    "\n",
    "# Define loss\n",
    "def cross_entropy_loss(y_, y):\n",
    "    epsilon = 1e-8\n",
    "    y_pred = torch.clamp(y_, epsilon, 1 - epsilon)\n",
    "    return -(y * torch.log(y_pred) + (1 - y) * torch.log(1 - y_pred))\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(X_train_tensor.shape[0]):\n",
    "        z = torch.matmul(X_train_tensor[i], weights) + bias\n",
    "        y_pred = torch.sigmoid(z)\n",
    "        loss = cross_entropy_loss(y_pred, y_train_tensor[i])\n",
    "        total_loss += loss\n",
    "    print(total_loss)\n",
    "    total_loss.backward()  # Backward pass on total (scalar) loss\n",
    "\n",
    "    # Gradient descent update\n",
    "    with torch.no_grad():\n",
    "        weights -= learning_rate * weights.grad\n",
    "        bias -= learning_rate * bias.grad\n",
    "\n",
    "        # Zero gradients\n",
    "        weights.grad.zero_()\n",
    "        bias.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8430c367-b999-44aa-97c4-2fe728f965fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4627],\n",
       "        [0.4433],\n",
       "        [0.4141],\n",
       "        [0.4099],\n",
       "        [0.3908],\n",
       "        [0.4654],\n",
       "        [0.4179],\n",
       "        [0.5111],\n",
       "        [0.4817],\n",
       "        [0.4740]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,num_features):\n",
    "        super().__init__()\n",
    "        self.linear=nn.Linear(num_features,1)\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "    def forward(self,features):\n",
    "        out=self.linear(features)\n",
    "        out=self.sigmoid(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "features=torch.rand(10,5)\n",
    "model=Model(features.shape[1])\n",
    "model(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "18b49448-92d0-4985-80b7-31676ed0ede6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.7031,  0.8324],\n",
      "        [ 0.9933, -1.1053]])\n",
      "tensor([1, 1])\n",
      "tensor([[-1.2601,  0.8223],\n",
      "        [-0.7963, -0.8222]])\n",
      "tensor([0, 0])\n",
      "tensor([[-1.1009, -1.2419],\n",
      "        [ 1.0584, -0.8539]])\n",
      "tensor([0, 1])\n",
      "tensor([[ 1.1396, -0.1354],\n",
      "        [-1.1569, -0.9864]])\n",
      "tensor([1, 0])\n",
      "tensor([[-1.0610, -0.6310],\n",
      "        [ 0.2870,  1.7550]])\n",
      "tensor([0, 1])\n"
     ]
    }
   ],
   "source": [
    "#dataset and dataloader classes\n",
    "from sklearn.datasets import make_classification\n",
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "X,y=make_classification(\n",
    "    n_samples=10,\n",
    "    n_features=2,\n",
    "    n_informative=2,\n",
    "    n_redundant=0,\n",
    "    n_classes=2,\n",
    "    random_state=10)\n",
    "X=torch.tensor(X,dtype=torch.float32)\n",
    "y=torch.tensor(y,dtype=torch.long)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,features,labels):\n",
    "        self.features=features\n",
    "        self.labels=labels\n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]\n",
    "    def __getitem__(self,index):\n",
    "        return self.features[index],self.labels[index]\n",
    "dataset=CustomDataset(X,y)\n",
    "len(dataset)\n",
    "dataset[0]\n",
    "dataloader=DataLoader(dataset,batch_size=2,shuffle=True)\n",
    "for batch_features,batch_labels in dataloader:\n",
    "    print(batch_features)\n",
    "    print(batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4415136b-5597-4bfa-af6f-3f03dd76c933",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
